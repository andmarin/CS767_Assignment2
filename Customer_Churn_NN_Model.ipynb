{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marinand\\AppData\\Local\\Temp\\ipykernel_37188\\535483469.py:28: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(binary_columns, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "\n",
    "# Drop Customer ID\n",
    "df = df.drop(columns=['customerID'])\n",
    "\n",
    "# Convert binary columns to numerical values\n",
    "binary_columns = {\n",
    "    'gender': {'Male': 1, 'Female': 0},\n",
    "    'Partner': {'Yes': 1, 'No': 0},\n",
    "    'Dependents': {'Yes': 1, 'No': 0},\n",
    "    'PhoneService': {'Yes': 1, 'No': 0},\n",
    "    'PaperlessBilling': {'Yes': 1, 'No': 0},\n",
    "    'Churn': {'Yes': 1, 'No': 0}\n",
    "}\n",
    "df.replace(binary_columns, inplace=True)\n",
    "\n",
    "# Replace empty strings with NaN and drop missing values\n",
    "df.replace(' ', pd.NA, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert numeric columns to appropriate data types\n",
    "numeric_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric)\n",
    "\n",
    "# Separate features and target\n",
    "y = df['Churn']\n",
    "X = df.drop(columns=['Churn'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps for numerical and categorical data\n",
    "numeric_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = ['MultipleLines', 'InternetService', 'OnlineSecurity', \n",
    "                        'OnlineBackup', 'DeviceProtection', 'TechSupport', \n",
    "                        'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "categorical_transformer = OneHotEncoder(drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[ \n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Build the pipeline without the classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit the pipeline on training data and transform both train and test data\n",
    "X_train_processed = pipeline.fit_transform(X_train)\n",
    "X_test_processed = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through Parameter combinations to see best performing combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "layer_sizes = [32, 48, 64]\n",
    "dropout_rates = [0.2, 0.3]\n",
    "learning_rates = [0.0001, 0.001, 0.005, 0.01, 0.05]\n",
    "batch_sizes = [16, 32, 64]\n",
    "optimizers = {'adam': Adam, 'rmsprop': RMSprop}\n",
    "epochs = 5  # Start with a small number of epochs for quick testing\n",
    "\n",
    "# Table to store results\n",
    "results = []\n",
    "\n",
    "# Loop over all combinations of parameters\n",
    "for layer_size in layer_sizes:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for learning_rate in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                for opt_name, opt_class in optimizers.items():\n",
    "                    # Build the model\n",
    "                    model = Sequential([\n",
    "                        Dense(layer_size, activation='relu', input_shape=(X_train_processed.shape[1],)),\n",
    "                        Dropout(dropout_rate),\n",
    "                        Dense(int(layer_size / 2), activation='relu'),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                    ])\n",
    "                    \n",
    "                    # Compile the model with dynamic optimizer\n",
    "                    optimizer = opt_class(learning_rate=learning_rate)\n",
    "                    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "                    \n",
    "                    # Train the model\n",
    "                    model.fit(X_train_processed, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                    \n",
    "                    # Evaluate on test data\n",
    "                    y_pred = (model.predict(X_test_processed) > 0.5).astype(\"int32\")\n",
    "                    f1 = f1_score(y_test, y_pred)\n",
    "                    \n",
    "                    # Store results\n",
    "                    results.append({\n",
    "                        'Layer Size': layer_size,\n",
    "                        'Dropout Rate': dropout_rate,\n",
    "                        'Learning Rate': learning_rate,\n",
    "                        'Batch Size': batch_size,\n",
    "                        'Optimizer': opt_name,\n",
    "                        'F1 Score': f1\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Layer Size  Dropout Rate  Learning Rate  Batch Size Optimizer  F1 Score\n",
      "167          64           0.3         0.0050          64   rmsprop  0.602410\n",
      "101          48           0.3         0.0010          64   rmsprop  0.599179\n",
      "72           48           0.2         0.0050          16      adam  0.597333\n",
      "48           32           0.3         0.0100          16      adam  0.591700\n",
      "38           32           0.3         0.0010          32      adam  0.591160\n",
      "..          ...           ...            ...         ...       ...       ...\n",
      "5            32           0.2         0.0001          64   rmsprop  0.285106\n",
      "55           32           0.3         0.0500          16   rmsprop  0.249443\n",
      "35           32           0.3         0.0001          64   rmsprop  0.244989\n",
      "34           32           0.3         0.0001          64      adam  0.147971\n",
      "4            32           0.2         0.0001          64      adam  0.010610\n",
      "\n",
      "[180 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame and sort by F1 Score\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='F1 Score', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model again with 1 set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights dictionary: {0: 0.6809927360774818, 1: 1.8812709030100334}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Create a dictionary for class weights\n",
    "class_weights_dict = {int(label): weight for label, weight in zip(np.unique(y_train), class_weights)}\n",
    "\n",
    "# Print the class weights dictionary for verification\n",
    "print(\"Class weights dictionary:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values  # Convert to a plain numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marinand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7090 - loss: 0.5434\n",
      "Epoch 2/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7518 - loss: 0.5007\n",
      "Epoch 3/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7540 - loss: 0.4829\n",
      "Epoch 4/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - accuracy: 0.7616 - loss: 0.4860\n",
      "Epoch 5/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.7638 - loss: 0.4855\n",
      "Epoch 6/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7505 - loss: 0.4803\n",
      "Epoch 7/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7565 - loss: 0.4957\n",
      "Epoch 8/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7675 - loss: 0.4750\n",
      "Epoch 9/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.7603 - loss: 0.4743\n",
      "Epoch 10/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950us/step - accuracy: 0.7671 - loss: 0.4831\n",
      "Epoch 11/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7636 - loss: 0.4715\n",
      "Epoch 12/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7655 - loss: 0.4733\n",
      "Epoch 13/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.4712\n",
      "Epoch 14/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7658 - loss: 0.4751\n",
      "Epoch 15/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7689 - loss: 0.4698\n",
      "Epoch 16/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7641 - loss: 0.4737\n",
      "Epoch 17/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - accuracy: 0.7787 - loss: 0.4668\n",
      "Epoch 18/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - accuracy: 0.7808 - loss: 0.4677\n",
      "Epoch 19/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.7821 - loss: 0.4627\n",
      "Epoch 20/20\n",
      "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.7745 - loss: 0.4608\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1 Score on test set: 0.5971143174250833\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "layer_size = 32\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(layer_size, activation='relu', input_shape=(X_train_processed.shape[1],)),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(int(layer_size / 2), activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(X_train_processed, y_train, epochs=epochs, batch_size=batch_size, \n",
    "          verbose=1, class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = (model.predict(X_test_processed) > 0.5).astype(\"int32\")\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Output F1 Score\n",
    "print(\"F1 Score on test set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Sample 1:\n",
      "Original Features:\n",
      "gender                             0\n",
      "SeniorCitizen                      1\n",
      "Partner                            0\n",
      "Dependents                         0\n",
      "tenure                            15\n",
      "PhoneService                       1\n",
      "MultipleLines                     No\n",
      "InternetService          Fiber optic\n",
      "OnlineSecurity                    No\n",
      "OnlineBackup                      No\n",
      "DeviceProtection                  No\n",
      "TechSupport                       No\n",
      "StreamingTV                      Yes\n",
      "StreamingMovies                  Yes\n",
      "Contract              Month-to-month\n",
      "PaperlessBilling                   1\n",
      "PaymentMethod       Electronic check\n",
      "MonthlyCharges                  91.5\n",
      "TotalCharges                  1400.3\n",
      "Name: 3469, dtype: object\n",
      "Original Target (True): 0\n",
      "Predicted Output: 1\n",
      "----------------------------------------\n",
      "Sample 2:\n",
      "Original Features:\n",
      "gender                           1\n",
      "SeniorCitizen                    0\n",
      "Partner                          1\n",
      "Dependents                       0\n",
      "tenure                           3\n",
      "PhoneService                     1\n",
      "MultipleLines                   No\n",
      "InternetService                DSL\n",
      "OnlineSecurity                  No\n",
      "OnlineBackup                    No\n",
      "DeviceProtection               Yes\n",
      "TechSupport                    Yes\n",
      "StreamingTV                     No\n",
      "StreamingMovies                 No\n",
      "Contract            Month-to-month\n",
      "PaperlessBilling                 1\n",
      "PaymentMethod         Mailed check\n",
      "MonthlyCharges                55.8\n",
      "TotalCharges                154.55\n",
      "Name: 1443, dtype: object\n",
      "Original Target (True): 0\n",
      "Predicted Output: 0\n",
      "----------------------------------------\n",
      "Sample 3:\n",
      "Original Features:\n",
      "gender                             0\n",
      "SeniorCitizen                      0\n",
      "Partner                            0\n",
      "Dependents                         0\n",
      "tenure                            31\n",
      "PhoneService                       1\n",
      "MultipleLines                     No\n",
      "InternetService          Fiber optic\n",
      "OnlineSecurity                   Yes\n",
      "OnlineBackup                      No\n",
      "DeviceProtection                 Yes\n",
      "TechSupport                      Yes\n",
      "StreamingTV                       No\n",
      "StreamingMovies                   No\n",
      "Contract              Month-to-month\n",
      "PaperlessBilling                   0\n",
      "PaymentMethod       Electronic check\n",
      "MonthlyCharges                  87.6\n",
      "TotalCharges                 2724.25\n",
      "Name: 5969, dtype: object\n",
      "Original Target (True): 0\n",
      "Predicted Output: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get 3 random indices from the test set\n",
    "random_indices = np.random.choice(X_test.index, size=3, replace=False)\n",
    "\n",
    "# Select the corresponding input features and target labels\n",
    "X_samples = X_test.loc[random_indices]\n",
    "y_samples = y_test.loc[random_indices]\n",
    "\n",
    "# Apply the preprocessing pipeline to the selected test samples\n",
    "X_samples_processed = pipeline.transform(X_samples)\n",
    "\n",
    "# Get the model's predictions for these samples\n",
    "y_pred_samples = (model.predict(X_samples_processed) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Display the original input features (before transformation) and the corresponding predictions\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(\"Original Features:\")\n",
    "    print(X_samples.iloc[i])\n",
    "    print(\"Original Target (True):\", y_samples.iloc[i])\n",
    "    print(\"Predicted Output:\", y_pred_samples[i][0])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
